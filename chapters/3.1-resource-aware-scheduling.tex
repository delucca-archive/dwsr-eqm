\subsection{Resource-aware scheduling}
\label{subsec:resource-aware-scheduling}

Pupykina and Agosta~\cite{pupykina2019} present a broad overview of the memory management field until 2019.
According to the authors, the main challenge in predicting memory usage is knowing the memory access patterns within an application.
Most current research focuses on using machine learning techniques to bypass this problem.
Although this approach has been successful, it can only predict the overall resource usage of a whole workload rather than the resource usage of a single task.

The observation made by Pupykina and Agosta~\cite{pupykina2019} is visible while evaluating recent work.
Most of the research done so far focuses only on the scheduler perspective, using memory usage history to predict the expected resource requirements of a given cluster.

E. R. Rodrigues \etal~\cite{rodrigues2016} present a machine learning model that can easily be integrated into a scheduler to predict the resource requirements of a given task when executing on a cluster.
Their work mainly focuses on the scheduler perspective since it uses past executions by the user to predict the resource requirements for future jobs.
While submitting a new job to the scheduler, the user must provide a manual estimate.
The authors use this estimate, alongside past executions by the same user, to predict the actual resource requirements of the job by using a polling strategy with many different machine-learning models.
On their pool, they use linear models, multilayer perceptrons, k-nearest neighbors, decision trees, and random forests.
Although effective, this approach is vulnerable to spurious correlations since past executions by the same user from other jobs can influence the prediction.

T. Mehmood \etal~\cite{mehmood2018} propose an ensemble machine learning model that can predict the expected resource usage of a cloud provider.
The workload prediction can refer to various aspects such as job inter-arrival time, aggregated resource request, CPU usage, memory usage, VM load, or host load. 
The proposed solution has three modules: Resource Manager, Prediction module, and Resource Allocator.
The Resource Manager maintains a database of resource usage information obtained from monitoring physical resources and task requests.
The Prediction module uses an ensemble model based on the stacking mechanism, which includes K Nearest Neighbor and Decision Tree as base classifiers.
The Resource Allocator makes more informed decisions based on the output prediction, defined by high, medium, and low usage.
Like E. R. Rodrigues \etal~\cite{rodrigues2016}, T. Mehmood \etal~\cite{mehmood2018} uses historical data to predict the resource requirements at a given time.
As the input data to train the machine learning model, T. Mehmood \etal~\cite{mehmood2018} uses a dataset provided by Google containing the trace data of many jobs executed on the Google Cloud Platform.

In contrast to both T. Mehmood \etal~\cite{mehmood2018} and E. R. Rodrigues \etal~\cite{rodrigues2016}, Phung \etal~\cite{phung2021} propose a heuristic approach without using past executions to achieve a smaller upper-bound on resource allocation.
Instead of creating a model to predict the resource requirements of a given job, the authors use a trial and error method to increase the available resources for a given job until it reaches the point where the job stops failing.
Alongside that approach,  Phung \etal~\cite{phung2021} divided the results into different levels considering the amount of information about the task provided by the user.
According to their findings, the information the user provides about the task affects how well the scheduler can allocate the appropriate resources.

On the other hand, Fang, Wang, and Sun~\cite{fang2018} take a different approach.
The authors used a statistical method called least squares to execute task scheduling based on the prediction of memory utilization and optimize Hadoop~\cite{hadoop} parameters.
The experiments conducted in this paper involve varying data block sizes and degrees of map parallelism.
The results of these experiments demonstrate that the system's performance improves as the degree of map parallelism parameter increases until it reaches a certain threshold.
Beyond this threshold, the performance declines due to the increased memory pressure on the compute node.
Finally, the paper introduces a task allocation mechanism that uses historical memory usage data of compute nodes and a least squares simulation curve to predict memory usage in the next phase.
The scheduling of new tasks on the cluster is associated with this.
By doing so, the mechanism aims to maximize the efficient use of memory space while preventing excessive memory pressure and subsequent reduction in computational performance.

Results show that the task allocation mechanism based on memory utilization prediction outperforms Hadoop's default task allocation mechanism. The performance improvement is achieved by dynamically adjusting the allocation of tasks according to the memory state of the nodes. The proposed mechanism reduces the average execution time by 6.625 seconds and enhances the performance by 4.25% compared to Hadoop's default task allocation mechanism. This demonstrates that the task scheduling mechanism based on the prediction of memory utilization has the potential to optimize the performance of Hadoop-based systems.
Instead of using historical data to predict the exact amount of resources required, the authors use a machine learning model to predict the current memory pressure of a given cluster.
Their research is coupled with the Hadoop~\cite{hadoop} framework and analyzes the status of all active jobs before submitting a new one on the same cluster.
Instead of predicting the resource requirements of a single job, this approach focuses on predicting the cluster's overall performance.

All the research discussed so far focus on handling proper resource allocation within the scheduler.
Although effective, they all require training with historical data of incoming jobs.
Both the required amount of historical data and the focus on resource allocation based on a heterogeneous pool of incoming jobs limit the usage of those approaches to large-scale clusters.
Using those approaches to predict the memory requirements of a single job is counterproductive since each job would require a different training dataset.

Considering this limitation, Ferreira da Silva~\etal~\cite{ferreira2013} proposes a machine learning model that uses a clustering approach to evaluate the input data and predict the resource requirements of a given job.
Their approach has high accuracy, but the results vary a lot based on the size and density of the input itself.

During their research, Ferreira da Silva \etal~\cite{ferreira2013} discovered that smaller datasets have a higher correlation rate between the parameters the clustering algorithm extracts and the resource consumption itself.
This observation is essential since it shows that it is possible to use input data features to predict a given job's resource requirements.
As the final goal of their research, Ferreira da Silva \etal~\cite{ferreira2013} created an online estimator tool designed explicitly to be used by schedulers to improve their scheduling process.

Like Ferreira da Silva \etal~\cite{ferreira2013}, B. T. Shealy \etal~\cite{shealy2021} try to extract features from the components of the execution in order to predict resource consumption.
Instead of gathering features only from the input data, B. T. Shealy \etal~\cite{shealy2021} also uses a set of user-defined run parameters as possible features to train the model.
This approach aims to create an algorithm-specific model that can predict the resource consumption of jobs.
The main limitation of this approach is the requirement of building a training dataset for each algorithm that the user wants to predict resource consumption.
Due to this fact, this approach is only suitable for the recurrent execution of specific algorithms, changing only the input data and a few run parameters.

Finally, A. V. Goponenko \etal~\cite{goponenko2020} focuses on a different perspective.
Their work proves resource-aware scheduling can be more effective than traditional scheduling algorithms.
They have integrated their tool into Slurm, considering resource requirements while scheduling new jobs.
During the research, the authors discovered that their test workload executed 9.4\% faster than the original scheduling, with more efficient usage of the cluster resources.
