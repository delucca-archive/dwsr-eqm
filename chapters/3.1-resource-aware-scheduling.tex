\subsection{Resource-aware scheduling}
\label{subsec:resource-aware-scheduling}

Pupykina and Agosta~\cite{pupykina2019} present a broad overview of the memory management field until 2019.
According to the authors, the main challenge in predicting memory usage is knowing the memory access patterns within an application.
Most current research focuses on using machine learning techniques to bypass this problem.
Although this approach has been successful, it can only predict the overall resource usage of a whole workload rather than the resource usage of a single task.

The observation made by Pupykina and Agosta~\cite{pupykina2019} is visible while evaluating recent work.
Most of the research done so far focuses only on the scheduler perspective, using memory usage history to predict the expected resource requirements of a given cluster.

E. R. Rodrigues \etal~\cite{rodrigues2016} present a machine learning model that can easily be integrated into a scheduler to predict the resource requirements of a given task when executing on a cluster.
Their work mainly focuses on the scheduler perspective since it uses past executions by the user to predict the resource requirements for future jobs.
While submitting a new job to the scheduler, the user must provide a manual estimate.
The authors use this estimate, alongside past executions by the same user, to predict the actual resource requirements of the job by using a polling strategy with many different machine-learning models.
On their pool, they use linear models, multilayer perceptrons, k-nearest neighbors, decision trees, and random forests.
Although effective, this approach is vulnerable to spurious correlations since past executions by the same user from other jobs can influence the prediction.

T. Mehmood \etal~\cite{mehmood2018} propose an ensemble machine learning model that can predict the expected resource usage of a cloud provider.
The workload prediction can refer to various aspects such as job inter-arrival time, aggregated resource request, CPU usage, memory usage, VM load, or host load. 
The proposed solution has three modules: Resource Manager, Prediction module, and Resource Allocator.
The Resource Manager maintains a database of resource usage information obtained from monitoring physical resources and task requests.
The Prediction module uses an ensemble model based on the stacking mechanism, which includes K Nearest Neighbor and Decision Tree as base classifiers.
The Resource Allocator makes more informed decisions based on the output prediction, defined by high, medium, and low usage.
Like E. R. Rodrigues \etal~\cite{rodrigues2016}, T. Mehmood \etal~\cite{mehmood2018} uses historical data to predict the resource requirements at a given time.
As the input data to train the machine learning model, T. Mehmood \etal~\cite{mehmood2018} uses a dataset provided by Google containing the trace data of many jobs executed on the Google Cloud Platform.

In contrast to both T. Mehmood \etal~\cite{mehmood2018} and E. R. Rodrigues \etal~\cite{rodrigues2016}, Phung \etal~\cite{phung2021} propose a heuristic approach without using past executions to achieve a smaller upper-bound on resource allocation.
Instead of creating a model to predict the resource requirements of a given job, the authors use a trial and error method to increase the available resources for a given job until it reaches the point where the job stops failing.
Alongside that approach,  Phung \etal~\cite{phung2021} divided the results into different levels considering the amount of information about the task provided by the user.
According to their findings, the information the user provides about the task affects how well the scheduler can allocate the appropriate resources.

On the other hand, Fang, Wang, and Sun~\cite{fang2018} take a different approach.
The authors used a statistical method called least squares to execute task scheduling based on the prediction of memory utilization and optimize Hadoop~\cite{hadoop} parameters.
The experiments conducted in this paper involve varying data block sizes and degrees of map parallelism.
The results of these experiments demonstrate that the system's performance improves as the degree of map parallelism parameter increases until it reaches a certain threshold.
Beyond this threshold, the performance declines due to the increased memory pressure on the compute node.
Finally, the paper introduces a task allocation mechanism that uses historical memory usage data of compute nodes and a least squares simulation curve to predict memory usage in the next phase.
The scheduling of new tasks on the cluster is associated with this.
By doing so, the mechanism aims to maximize the efficient use of memory space while preventing excessive memory pressure and subsequent reduction in computational performance.

All the research discussed so far focus on handling proper resource allocation within the scheduler.
Although effective, they all require training with historical data of incoming jobs.
Both the required amount of historical data and the focus on resource allocation based on a heterogeneous pool of incoming jobs limit the usage of those approaches to large-scale clusters.
Using those approaches to predict the memory requirements of a single job is counterproductive since each job would require a different training dataset.

Considering this limitation, Ferreira da Silva~\etal~\cite{ferreira2013} proposes using the \ac{MAPE-K}~\cite{mapek} loop as an online estimator based on the workflow execution characteristics.
The authors proposed a manual profiling process identifying correlations between task need parameters and input data size.
Considering a scenario with no correlations, the authors proposed a density-based clustering to identify groups of high-density areas.
Their approach has high accuracy but requires a manual profiling process to identify the correlations between the input data and the task needs.

During their research, Ferreira da Silva \etal~\cite{ferreira2013} discovered that smaller datasets have a higher correlation rate between the parameters the clustering algorithm extracts and the resource consumption itself.
This observation is essential since it shows that it is possible to use input data features to predict a given job's resource requirements.

Like Ferreira da Silva \etal~\cite{ferreira2013}, B. T. Shealy \etal~\cite{shealy2021} try to extract features from the components of the execution in order to predict resource consumption.
Instead of gathering features only from the input data, B. T. Shealy \etal~\cite{shealy2021} also uses a set of user-defined run parameters as possible features to train the model.
This approach aims to create an algorithm-specific model that can predict the resource consumption of jobs.
The main limitation of this approach is the requirement of building a training dataset for each algorithm that the user wants to predict resource consumption.
Due to this fact, this approach is only suitable for the recurrent execution of specific algorithms, changing only the input data and a few run parameters.

Finally, A. V. Goponenko \etal~\cite{goponenko2020} focuses on a different perspective.
The adaptive scheduling strategy presented by the authors aims to optimize the execution of jobs by considering the resource requirements of each job and the real-time utilization of resources.
The strategy is focused on spreading the system's access evenly over time to achieve better performance.
The approach's core idea is workload-dependent utilization targets, different from the conventional constant resource utilization limit.
During the research, the authors discovered that their test workload executed 9.4\% faster than the original scheduling, with more efficient usage of the cluster resources.
In general, A. V. Goponenko \etal~\cite{goponenko2020} estimate the job duration and rate based on previous runs of similar jobs, corresponding to the current system state.
They use the estimated values as resource requirements for the job to maintain the system throughput.
By applying this adaptive scheduling strategy, the scheduler can dynamically adjust the resource allocation based on the current workload, potentially improving the execution time of the job queue.
The authors tested their approach through simulations, demonstrating this strategy's feasibility and efficiency.
It is important to note that this work does not involve a machine learning model but rather an adaptive scheduling strategy evaluated through simulations.
