\subsection{Potential risks and limitations}
\label{subsec:potential-risks-and-limitations}

This section will discuss the limitations of the proposed solution considering the current state of the research.

\subsubsection{Memory usage variance based on the input data}

Some algorithms may have memory usage that varies depending on the input data.
Correlating features from the input data may not be possible if the algorithm structure contains control statements that drastically change the memory usage of the algorithm.

It is essential to mention that this limitation only happens if the control statement directly affects memory allocation.
Even if the algorithm has many control statements that change the execution flow drastically, this limitation can be ignored if the memory allocation of the algorithm happens before those. 

Either way, all the seismic operators and machine learning models used are tensorial algorithms, meaning their execution flow would not vary based on the input data.
Therefore, this limitation may not affect the research itself.

\subsubsection{Unpredictable bottlenecks}

There are two possible memory bottlenecks in the proposed solution: \ac{CPU} and \ac{GPU} memory consumption.
Seismic operators are likely to be \ac{GPU}-memory bounded, but this statement is still pending validation through experiments.

If the algorithms can be both \ac{GPU} memory and \ac{CPU} memory bounded, then the proposed solution must be able to handle both scenarios.

\subsubsection{Language-agnosticity}

The proposed solution currently focuses on using Python since it is the language used for the seismic operators and Dask~\cite{dask} itself.

Although Python is a popular language in the scientific community, there may be other languages of choice for some researchers.
This solution may need to be more language-agnostic at the moment.
However, it is possible to improve the solution to accept algorithms from any language in the future.

In the case of the domain-specific model, it is possible to improve the training structure to accept tasks from any language by decoupling both the feature extraction and the execution of the graph from the training process.

In the case of the generic model, it is possible to improve the feature-extraction part to allow more languages as well.
Since the only difference between the two strategies is allowing to extract features from the source code, a specialized feature extractor per language should be enough.

\subsubsection{DASK's memory management}

Since \ac{DASF}~\cite{dasf} uses Dask~\cite{dask} under the hood, it is crucial to understand how Dask~\cite{dask} manages memory.
Considering all experiments conducted so far, the library has an automatic chunking feature that executes the proper data partitioning as long as the developer can define the maximum chunk size in bytes.

This feature seems promising and may solve most of the challenges related to automatic data partitioning.
Nevertheless, conducting proper experiments to understand exactly how this feature works and its limitations is essential.
Depending on the results of such experiments, the proposed solution may change to work with Dask's~\cite{dask} memory management.
