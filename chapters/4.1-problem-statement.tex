\subsection{Problem statement}
\label{subsec:problem-statement}

The Discovery~\footnote{\url{https://discovery.ic.unicamp.br/}} laboratory, located at UNICAMP~\footnote{\url{https://ic.unicamp.br/}}, is working on a seismic analysis project with Petrobras~\footnote{\url{https://petrobras.com.br/}}.
This project relies heavily on machine learning and seismic operators for its computing graphs.
However, the input of these graphs is a massive seismic dataset that can contain terabytes of data.
Even supercomputers do not have enough memory to handle the computation on a single node.
Therefore, usually the execution is distributed by using data parallelism.

To facilitate this process, our laboratory implemented a framework called DASF~\cite{dasf} that simplifies the development of data-parallel computing graphs using DASK~\cite{dask} under the hood.
DASF~\cite{dasf} has an input parameter called \textbf{block\_size} that defines the size of each seismic block used during data parallelism.
However, setting this parameter can be challenging because it required finding the optimal relationship between it and the network overhead caused by it.

To illustrate this challenge, I present image~\ref{fig:block\_size} which contains three computing graphs receiving input data from a seismic dataset.
The first graph shows the input data as the entire dataset, which requires a significant amount of memory to execute.
The second graph divides the data into thousands of small parts, reducing the memory requirement but adding network overhead.
The third graph divides the data into a smaller number of parts, minimizing both network and memory requirements.

% \begin{figure}[h]
% \label{fig:block-size}
% \includegraphics[width=8cm]{block_size.png}
% \end{figure}

While executing the graph, the developer must manually set the block\_size parameter.
Since Petrobras use subcomputers to execute those graphs, setting a large number may lead to memory issues causing a signifcant delay due the trial-and-error nature of the usual execution.
On the other hand, setting a small number may increase the execution time due to network overhead.
Since Petrobras have a large number of graphs to execute, and each graph usually takes a long time to execute, I need to find a way to optimize the block\_size parameter.

DASK~\cite{dask} provides an automatic chunking feature, but it relies on the chunk\_size parameter, which is a static parameter define prior to execution.
Since the developer do need to define this parameter prior to the execution, they can't rely on DASK~\cite{dask} auto chunking feature to automatically split the data, but they can rely on the automatic chunking if they figure out the ideal chunk\_size prior to the execution.

Based on this assumption, if someone predicts the memory usage of the graph that person can use the DASK~\cite{dask} auto chunking feature to automatically split the data into the ideal number of chunks.
Therefore, this research aims to develop a memory usage prediction model that can help us determine the optimal block\_size parameter during execution.
This will help us optimize resource utilization and minimize waiting and execution time.
The model will provide a comprehensive understanding of memory usage patterns for different block sizes and contribute to the development of more efficient resource allocation strategies in large-scale clusters.
